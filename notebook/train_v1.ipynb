{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "\n",
    "import spacy\n",
    "import statistics\n",
    "import torchtext\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.metrics import Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_eng = spacy.load('en_core_web_sm')\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold, spacy_eng=None):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        if spacy_eng==None:\n",
    "            self.spacy_eng = spacy.load('en_core_web_sm')\n",
    "        else:\n",
    "            self.spacy_eng = spacy_eng\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenizer_eng(self, text):\n",
    "        return [tok.text.lower() for tok in self.spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, caption_file, transform=None, freq_threshold=5,\n",
    "                 train=True, split_val=0.2):\n",
    "        self.root_dir = root_dir\n",
    "        self.caption_file = caption_file\n",
    "        self.df = pd.read_csv(caption_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.df['caption'].tolist())\n",
    "        \n",
    "        self.train = train \n",
    "        self.split_val = split_val\n",
    "        self._do_split_train_valid()\n",
    "        \n",
    "#         # Get img, caption columns\n",
    "#         self.imgs = self.df[\"image\"]\n",
    "#         self.captions = self.df[\"caption\"]\n",
    "\n",
    "        # Initialize vocabulary and build vocab\n",
    "\n",
    "        \n",
    "    def _do_split_train_valid(self):\n",
    "        imgs_train, imgs_valid, caps_train, caps_valid = train_test_split(\n",
    "            self.df[\"image\"], self.df[\"caption\"], \n",
    "            test_size=self.split_val, random_state=16\n",
    "        )\n",
    "        \n",
    "        if self.train:\n",
    "            self.imgs = imgs_train\n",
    "            self.captions = caps_train\n",
    "        else:\n",
    "            self.imgs = imgs_valid\n",
    "            self.captions = caps_valid\n",
    "            \n",
    "        self.imgs = self.imgs.tolist()\n",
    "        self.captions = self.captions.tolist()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def _numericalized_caption(self, caption):\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return numericalized_caption\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        ncaption = self._numericalized_caption(caption)\n",
    "\n",
    "        return img, torch.tensor(ncaption)\n",
    "\n",
    "\n",
    "class CaptionCollate:\n",
    "    def __init__(self, pad_idx, batch_first=True):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flickr8k_dataloader(root_folder, caption_file, transform, train=True,\n",
    "                        batch_size=32, num_workers=8, shuffle=True, pin_memory=True):\n",
    "    \n",
    "    dataset = FlickrDataset(root_folder, caption_file, transform=transform, train=train)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                            shuffle=shuffle, pin_memory=pin_memory, \n",
    "                            collate_fn=CaptionCollate(pad_idx=pad_idx))\n",
    "    \n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "root_dir = \"data/flickr8k/images/\"\n",
    "caption_file = \"data/flickr8k/captions.txt\"\n",
    "\n",
    "train_loader, trainset = flickr8k_dataloader(root_dir, caption_file, transform=train_transform, train=True)\n",
    "valid_loader, validset = flickr8k_dataloader(root_dir, caption_file, transform=valid_transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'the', 'clowns', 'are', 'striking', 'a', 'pose', 'for', 'the', 'camera', '.', '<EOS>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, cap = trainset[200]\n",
    "print([trainset.vocab.itos[token] for token in cap.tolist()])\n",
    "\n",
    "len(trainset), len(validset)\n",
    "type(trainset.vocab.stoi[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'a', 'black', 'dog', 'on', 'a', 'rocky', 'beach', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, caps = next(iter(valid_loader))\n",
    "print([trainset.vocab.itos[token] for token in caps[0].tolist()])\n",
    "\n",
    "imgs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.resnet = models.resnet34(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ImageCaptionNet(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(ImageCaptionNet, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionTask(pl.LightningModule):\n",
    "    def __init__(self, model, optimizers, criterion, scheduler=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.metric = Accuracy()\n",
    "        \n",
    "    def forward(self, imgs, captions):\n",
    "        outputs = self.model(imgs, captions[:-1])\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def shared_step(self, batch, batch_idx):\n",
    "        imgs, captions = batch\n",
    "        outputs = self.model(imgs, captions[:-1])\n",
    "        \n",
    "        outputs_preprocess = outputs.reshape(-1, outputs.shape[2])\n",
    "        captions_preprocess = captions.reshape(-1)\n",
    "        loss = criterion(outputs_preprocess, captions_preprocess)\n",
    "#         acc = (output.argmax(1) == cls).sum().item()\n",
    "        acc = self.metric(outputs_preprocess.argmax(1), captions_preprocess)\n",
    "        return loss, acc\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch, batch_idx)\n",
    "        result = pl.TrainResult(loss)\n",
    "#         result.log_dict({'trn_loss': loss})\n",
    "        result.log_dict({'trn_loss': loss, 'trn_acc':acc})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch, batch_idx)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "#         result.log_dict({'val_loss': loss})\n",
    "        result.log_dict({'val_loss': loss, 'val_acc': acc})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.scheduler:\n",
    "            return [self.optimizer], [self.scheduler]\n",
    "        return self.optimizer\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import save_checkpoint, load_checkpoint, print_examples\n",
    "# from get_loader import get_loader\n",
    "# from model import CNNtoRNN\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(trainset.vocab)\n",
    "num_layers = 1\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2994"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model, loss etc\n",
    "pad_index = trainset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "model = ImageCaptionNet(embed_size, hidden_size, vocab_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../saved_model'\n",
    "# DEFAULTS used by the Trainer\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='checkpoint_on',\n",
    "    mode='min',\n",
    "    prefix='flickr8k_net_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger('../logs/flickr8k')\n",
    "task = ImageCaptionTask(model, optimizer, criterion)\n",
    "trainer = pl.Trainer(gpus=0, logger=tb_logger, checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(task, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
